{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GiorgiaAuroraAdorni/ML-bachelor-course-assignments-sp23/blob/main/assignment%201/deliverable/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "Student: Kelvin Likollari\n",
    "\n",
    "--- \n",
    "# IMPORTANT: all the submitted code should be in 2 cells\n",
    "1) How you trained, evaluated and saved your model\n",
    "2) How to load your model from a file, load the data and evaluate the model. Cell 2) should be running independently (even if cell 1 is not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations for test size: 0.001\n",
      "Intercept:  1.268806890202894\n",
      "Coefficients:  [ 0.         -0.04803627 -0.57083818  0.46504925  0.04017861]\n",
      "Mean squared error:  0.14265901480513626\n",
      "You have a test size of : 0.001, with a Mean Square Error of: 0.14265901480513626 and an error percentage of: -17.847015856365797\n",
      "observations for test size: 0.1\n",
      "Intercept:  1.2503923040128955\n",
      "Coefficients:  [ 0.         -0.03680377 -0.55664722  0.45826896  0.0346524 ]\n",
      "Mean squared error:  0.5523746852292406\n",
      "You have a test size of : 0.1, with a Mean Square Error of: 0.5523746852292406 and an error percentage of: -9515.323200792043\n",
      "observations for test size: 0.2\n",
      "Intercept:  1.2852593942090575\n",
      "Coefficients:  [ 0.         -0.0451978  -0.57474559  0.45765517  0.04029218]\n",
      "Mean squared error:  0.6443717113826114\n",
      "You have a test size of : 0.2, with a Mean Square Error of: 0.6443717113826114 and an error percentage of: 3168.2034937933986\n",
      "observations for test size: 0.3\n",
      "Intercept:  1.2667310499204747\n",
      "Coefficients:  [ 0.         -0.03573194 -0.57074131  0.48169006  0.03947641]\n",
      "Mean squared error:  0.638065284253748\n",
      "You have a test size of : 0.3, with a Mean Square Error of: 0.638065284253748 and an error percentage of: 2142.288515777071\n",
      "Errors for each test size: [-17.847015856365797, -9515.323200792043, 3168.2034937933986, 2142.288515777071]\n",
      "Non-Linear model Mean squared error:  0.022032199592091756\n",
      "You have a test size of : 0.001, with a Mean Square Error of: 0.022032199592091756 and an error percentage of: -2.756285791036604\n",
      "Non-Linear model Mean squared error:  0.03231884203417052\n",
      "You have a test size of : 0.1, with a Mean Square Error of: 0.03231884203417052 and an error percentage of: -556.7312109946721\n",
      "Non-Linear model Mean squared error:  0.03544803501979555\n",
      "You have a test size of : 0.2, with a Mean Square Error of: 0.03544803501979555 and an error percentage of: 174.28851455451655\n",
      "Non-Linear model Mean squared error:  0.039132504576758036\n",
      "You have a test size of : 0.3, with a Mean Square Error of: 0.039132504576758036 and an error percentage of: 131.38642270191804\n",
      "Everything useful\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load data \n",
    "url = 'https://drive.switch.ch/index.php/s/TeDwnbYsBKRuJjv/download'\n",
    "response = requests.get(url)\n",
    "data = np.load(io.BytesIO(response.content))\n",
    "\n",
    "# Alternatively yo can load the data from file\n",
    "# data_path = '../data/data.npz' # path to the .npz file storing the data\n",
    "# data = np.load(data_path)\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# T1\n",
    "\n",
    "# function to split the data into training and testing set \n",
    "def split_data(x, y, test_size, random_state):\n",
    "    # x = input data, y are the target data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, shuffle=True, random_state=random_state)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# train-test size ratios: 100-0, 90-10, 80-20, 70-30 etc\n",
    "test_sizes = [0.001, 0.1, 0.2, 0.3]\n",
    "errors = []\n",
    "for test_size in test_sizes:\n",
    "    print(\"observations for test size:\", test_size)\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, test_size, random_state=40)\n",
    "    # create two new features, one for the training and one for the test set, by computing the square of the first 2 features in both sets\n",
    "    # and concatenating them with the original features to create the new augmented sets.\n",
    "    # f (x, θ) = θ0 + θ1 · x1 + θ2 · x2 + θ3 · sin(x2) + θ4 · x1 · x2\n",
    "\n",
    "    x_train_aug = np.c_[np.ones_like(x_train[:, 0]), x_train[:, 0], x_train[:, 1], np.sin(x_train[:, 1]), x_train[:, 0] * x_train[:, 1]]\n",
    "    x_test_aug = np.c_[np.ones_like(x_test[:, 0]), x_test[:, 0], x_test[:, 1], np.sin(x_test[:, 1]), x_test[:, 0] * x_test[:, 1]]\n",
    "   \n",
    "    # create a linear regression model with an intercept\n",
    "    # fit_intercept is set by default to true.\n",
    "    lrmodel = LinearRegression()\n",
    "\n",
    "    # fit the model to the training data.\n",
    "    lrmodel.fit(x_train_aug, y_train)\n",
    "\n",
    "    # print the intercept and the coefficients\n",
    "    print(\"Intercept: \", lrmodel.intercept_)\n",
    "    print(\"Coefficients: \", lrmodel.coef_)\n",
    "\n",
    "    # predict the targets for the test data\n",
    "    y_pred = lrmodel.predict(x_test_aug)\n",
    "\n",
    "    # print the mean squared error\n",
    "    mse = ((y_pred - y_test) ** 2).mean()\n",
    "    # print(\"pred issssssss:\", y_pred, \"test issssss:\", y_test)\n",
    "    print(\"Mean squared error: \", mse)\n",
    "\n",
    "    # calculate error percentage \n",
    "    error_percentage = (mse / y_test.mean()) * 100\n",
    "    errors.append(error_percentage)\n",
    "\n",
    "    print(f\"You have a test size of : {test_size}, with a Mean Square Error of: {mse} and an error percentage of: {error_percentage}\")\n",
    "\n",
    "print(\"Errors for each test size:\", errors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# T2\n",
    "# solving the above regression problem using a non-linear model\n",
    "\n",
    "# Import libraries\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load data \n",
    "url = 'https://drive.switch.ch/index.php/s/TeDwnbYsBKRuJjv/download'\n",
    "response = requests.get(url)\n",
    "data = np.load(io.BytesIO(response.content))\n",
    "\n",
    "# Alternatively yo can load the data from file\n",
    "# data_path = '../data/data.npz' # path to the .npz file storing the data\n",
    "# data = np.load(data_path)\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# function to split the data into training and testing set \n",
    "def split_data(x, y, test_size, random_state):\n",
    "    # x = input data, y are the target data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, shuffle=True, random_state=random_state)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "test_sizes = [0.001, 0.1, 0.2, 0.3]\n",
    "nlr_errors = []\n",
    "for test_size in test_sizes:\n",
    "    x_train, x_test, y_train, y_test = split_data(x, y, test_size, random_state=40)\n",
    "\n",
    "    # create two new features, one for the training and one for the test set, by computing the square of the first 2 features in both sets\n",
    "    # and concatenating them with the original features to create the new augmented sets.\n",
    "    x_train_aug = np.c_[np.ones_like(x_train[:, 0]), x_train[:, 0], x_train[:, 1], np.sin(x_train[:, 1]), x_train[:, 0] * x_train[:, 1]]\n",
    "    x_test_aug = np.c_[np.ones_like(x_test[:, 0]), x_test[:, 0], x_test[:, 1], np.sin(x_test[:, 1]), x_test[:, 0] * x_test[:, 1]]\n",
    "   \n",
    "\n",
    "    # create a support vector regression model\n",
    "    # Radial Basis Function kernel\n",
    "    svrmodel = SVR(C=100, gamma=.1)\n",
    "\n",
    "    # fit the model to the training data.\n",
    "    svrmodel.fit(x_train_aug, y_train)\n",
    "\n",
    "    # predict the targets for the test data\n",
    "    y_pred = svrmodel.predict(x_test_aug)\n",
    "\n",
    "    # print the mean squared error\n",
    "    mse = ((y_pred - y_test) ** 2).mean()\n",
    "    print(\"Non-Linear model Mean squared error: \", mse)\n",
    "\n",
    "    # calculate error percentage\n",
    "    error_percentage = (mse / y_test.mean()) * 100\n",
    "    nlr_errors.append(error_percentage)\n",
    "\n",
    "    print(f\"You have a test size of : {test_size}, with a Mean Square Error of: {mse} and an error percentage of: {error_percentage}\")\n",
    "\n",
    "\n",
    "# T3 (Bonus)\n",
    "\n",
    "print(\"Everything useful\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on how to use baseline model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(2000,)\n",
      "MSE on whole dataset: 0.022251897715738274\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import joblib\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluates the mean squared error between the values in y_true and the values\n",
    "    in y_pred.\n",
    "    ### YOU CAN NOT EDIT THIS FUNCTION ###\n",
    "    :param y_true: Numpy array, the true target values from the test set;\n",
    "    :param y_pred: Numpy array, the values predicted by your model.\n",
    "    :return: float, the mean squared error between the two arrays.\n",
    "    \"\"\"\n",
    "    print(y_true.shape)\n",
    "    print(y_pred.shape)\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    Loads a Scikit-learn model saved with joblib.dump.\n",
    "    This is just an example, you can write your own function to load the model.\n",
    "    Some examples can be found in src/utils.py.\n",
    "    :param filename: string, path to the file storing the model.\n",
    "    :return: the model.\n",
    "    \"\"\"\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load the data\n",
    "# This will be replaced with our private test data when grading the assignment\n",
    "\n",
    "# Load data from url\n",
    "url = 'https://drive.switch.ch/index.php/s/TeDwnbYsBKRuJjv/download'\n",
    "response = requests.get(url)\n",
    "data = np.load(io.BytesIO(response.content))\n",
    "\n",
    "# Alternatively yo can load the data from file\n",
    "# data_path = '../data/data.npz'\n",
    "# data = np.load(data_path)\n",
    "\n",
    "# x is a Numpy array of shape (n_samples, n_features) with the inputs\n",
    "x = data.f.x\n",
    "# y is a Numpy array of shape (n_samples, ) with the targets\n",
    "y = data.f.y\n",
    "\n",
    "# Load the trained model\n",
    "baseline_model_path = './baseline_model.pickle'\n",
    "baseline_model = load_model(baseline_model_path)\n",
    "\n",
    "# Change input\n",
    "x = np.c_[np.ones_like(x[:, 0]), x[:, 0], x[:, 1], np.sin(x[:, 1]), x[:, 0] * x[:, 1]]\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the given samples\n",
    "y_pred = baseline_model.predict(x)\n",
    "\n",
    "############################################################################\n",
    "# STOP EDITABLE SECTION: do not modify anything below this point.\n",
    "############################################################################\n",
    "\n",
    "# Evaluate the prediction using MSE\n",
    "mse = evaluate_predictions(y_pred, y)\n",
    "print(f'MSE on whole dataset: {mse}')\n",
    "\n",
    "# NOTE: NOW THIS CELL IS NOT WORKING SINCE YOU NEED TO CHANGE THE INPUT.\n",
    "# DO IT AND EVERYTHING RUNS SMOOTH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
